{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) and Time Series Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving forward, we will be using google colab to run our code. You can access google colab with this link: https://colab.research.google.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation and Relationship Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation and relationship analysis are important techniques in data science for understanding the association between variables. In this lesson, we will explore the concept of correlation, its types, and how to analyze relationships between variables using Python. We will use practical examples to demonstrate these concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation measures the statistical relationship between two variables. It helps us understand how changes in one variable are associated with changes in another variable. Correlation does not imply causation, but it indicates the strength and direction of the relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different correlation coefficients used to quantify the relationship between variables:\n",
    "\n",
    "Pearson correlation coefficient (r): It measures the linear relationship between two continuous variables. The value of r ranges from -1 to +1. A positive value indicates a positive linear relationship, a negative value indicates a negative linear relationship, and a value close to zero indicates no linear relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Example data\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([3, 4, 5, 6, 7])\n",
    "\n",
    "# Calculate Pearson correlation coefficient\n",
    "correlation_matrix = np.corrcoef(x, y)\n",
    "\n",
    "# since .corrcoef returns a matrix of the correlations\n",
    "# for different combinations of variables, we choose the index\n",
    "# [0, 1] to access the correlation for variable 1 (0th index) \n",
    "# and variable 2 (1st index)\n",
    "pearson_coefficient = correlation_matrix[0, 1]\n",
    "\n",
    "print(\"Pearson correlation coefficient:\", correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the above result, how would you describe the relationship between these data points?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style = \"background-color: yellow\">\n",
    "TODO: Pick two numeric data points from your own dataset that you think might be correlated. Use the above correlation calculation to quantify the relationship between those two points. In a sentence or two, write down how you would describe the relationship between the two data points.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize our correlation by seeing the scatterplot of the variables plotted against one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example data\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([3, 4, 5, 6, 7])\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.scatter(x, y)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Scatter Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we programmatically created a positive, linear relationship in the above example. That is why the correlation coefficient was a 1 and the scatter plot shows a clear positive, linear relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When interpreting correlation coefficients, recall these concepts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Positive Correlation: A positive correlation coefficient indicates that as one variable increases, the other variable tends to increase as well. The closer the value is to +1, the stronger the positive correlation.\n",
    "\n",
    "Negative Correlation: A negative correlation coefficient indicates that as one variable increases, the other variable tends to decrease. The closer the value is to -1, the stronger the negative correlation.\n",
    "\n",
    "No Correlation: A correlation coefficient close to zero indicates no linear relationship between the variables. However, it's important to note that there could still be a non-linear relationship or a relationship that is not captured by the correlation coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation analysis may be affected by missing values and outliers in the data, so be sure to consider the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing Values: Missing data can lead to biased correlation results. You can handle missing values by imputation techniques (e.g., mean, median, or regression imputation) or by removing observations with missing data, depending on the situation.\n",
    "\n",
    "Outliers: Outliers can have a significant impact on correlation coefficients. Consider identifying and handling outliers before performing correlation analysis. Techniques like winsorization, trimming, or using robust correlation measures can help mitigate the influence of outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the upcoming lessons, we will be diving into machine learning. Machine learning is a branch of artificial intelligence (AI) that focuses on developing algorithms and models that enable computers to learn and make predictions or decisions without being explicitly programmed. In simple terms, machine learning is about teaching computers how to learn from data and use that knowledge to perform tasks or make predictions.\n",
    "\n",
    "The 3 fundamental steps of Machine Learning can be described as first getting your data, then creating a model + training it with the existing data, and lastly using the model to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn, also known as sklearn, is a widely-used open-source machine learning library for Python. It provides a range of efficient tools and algorithms for various machine learning tasks, including classification, regression, clustering, dimensionality reduction, model selection, and preprocessing of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data\n",
    "\n",
    "First, we need to load our data set. Sklearn does have its own pre-loaded datasets that we can access. One such dataset is the \"Iris Dataset\", often referred to as the \"Hello World\" of Machine Learning. Recall that \"Hello World\" is synonymous with \"the first program you write in any programming language\". This dataset contains the sepal and pedal length of different flowers.\n",
    "\n",
    "Let's import it from sklearn's pre-loaded datasets and start with some basic exploratory analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load Iris Data\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Data\n",
    "\n",
    "Let's get a better understanding of the data by turning it into a pandas dataframe then using the `.head()` function to sample some rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's check the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Iris dataset, the target values are encoded as 0, 1, and 2. Here is the mapping:\n",
    "\n",
    "- 0: setosa\n",
    "- 1: versicolor\n",
    "- 2: virginica\n",
    "\n",
    "Let's add these columns to demonstrate how the genus of the flower is related to the different features of the plant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the target column to your DataFrame\n",
    "iris_df['target'] = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to find patterns in our data using pairplot.\n",
    "\n",
    "Pairplot visualizes given data to find the relationship between them where the variables can be continuous or categorical. Do you see any patterns here? Which combination(s) of features appear to be correlated, or perhaps help us group the plant by genus (the colors on the dots)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.pairplot(iris_df, hue='target')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, these different features appear to provide helpful groupings. Now, what if we wanted to **predict** the genus of a plant without knowing its genus? This is where Machine Learning can help.\n",
    "\n",
    "We will want to categorize this data with a type of regression called \"Logistic Regression\". Before we do any Machine Learning, we want to prepare our data. We will want to **train** a model where the output is labeled (ex. the data **has** the plant's genus) and **evaluate** the model to observe it's predictive ability.\n",
    "\n",
    "Given that our training data is **labeled** in the training data, that makes this model a \"Supervised\" Model. The main difference between supervised vs unsupervised learning is the need for labelled training data. Supervised machine learning relies on labelled input and output training data, whereas unsupervised learning processes unlabelled or raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# All columns except 'target'\n",
    "X = iris_df.drop('target', axis=1)\n",
    "\n",
    "# Only 'target' column\n",
    "y = iris_df['target']\n",
    "\n",
    "# The function train_test_split splits arrays or matrices into random train and test subsets. \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a machine learning model\n",
    "\n",
    "We will use a simple logistic regression model for this multi-class classification problem. Despite its name, logistic regression can be used in classification problems, where the goal is to predict a categorical dependent variable. The independent variables would be the features of the plant (petal length, sepal width, etc.) and the dependent variable would be the genus of the plant (categorical data that we represent numerically with setosa (0), versicolor (1), and virginica (1)).\n",
    "\n",
    "While we will go into how a machine learning model trains, a bit of the math behind it, and when to use which model in future lessons - the following is a brief over of Logistic Regression.\n",
    "\n",
    "Logistic regression measures the relationship between the categorical dependent variable and one or more independent variables by estimating probabilities using a logistic function. One of the main ideas behind Logistic Regression is the sigmoid function. When our input (z) is very large, sigmoid returns a value close to 1, and when our input is very small, sigmoid returns a value close to 0. It maps any real-valued number into another value between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(max_iter=200, random_state=42)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the target values for the test dataset\n",
    "\n",
    "Now that the model has been trained (on the **labeled** data with the flower's genus), it can be used to predict the species of iris flowers in the test dataset (which is **not labeled**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model\n",
    "\n",
    "Now, we need to see how well the model performs. One common metric for classification is accuracy, which is the proportion of test instances that were classified correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Model accuracy is: \", accuracy_score(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confusion matrix gives a better representation of what errors are slipping through with additional metrics.\n",
    "\n",
    "A confusion matrix is a table often used to describe the performance of a classification model on a set of data for which the true values are known. It essentially is a summary of prediction results on a classification problem.\n",
    "\n",
    "For a binary classification problem, the confusion matrix is a 2x2 table:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                     | Predicted Positive | Predicted Negative |\n",
    "|---------------------|--------------------|--------------------|\n",
    "| Actual Positive     | True Positive (TP) | False Negative (FN)|\n",
    "| Actual Negative     | False Positive (FP)| True Negative (TN) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- True Positives (TP): These are the correctly predicted positive values which means that the value of actual class is yes and the value of the predicted class is also yes.\n",
    "\n",
    "- True Negatives (TN): These are the correctly predicted negative values which means that the value of actual class is no and value of predicted class is also no.\n",
    "\n",
    "- False Positives (FP): When the actual class is no and predicted class is yes. Also known as \"Type I error\".\n",
    "\n",
    "- False Negatives (FN): When the actual class is yes but the predicted class in no. Also known as \"Type II error\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accuracy\n",
    "Accuracy is the most intuitive performance measure and it is simply a ratio of correctly predicted observations to the total observations.\n",
    "\n",
    "Accuracy = TP+TN/TP+FP+FN+TN\n",
    "\n",
    "##### Precision\n",
    "Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. High precision relates to the low false positive rate.\n",
    "\n",
    "Precision = TP/TP+FP\n",
    "\n",
    "##### Recall (Sensitivity)\n",
    "Recall (also known as Sensitivity, Hit Rate, or True Positive Rate) is the ratio of correctly predicted positive observations to the all observations in actual class.\n",
    "\n",
    "Recall = TP/TP+FN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now observe the confusion matrix for what our model predicted on the test data (after being trained on the training data).\n",
    "\n",
    "On the Y axis you will see the actual genus labels of the data, and on the X axis you will see the predicted genus (from the Logistic Regression Model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of a confusion matrix with some dummy data:\n",
    "\n",
    "|                     | Predicted Positive | Predicted Negative |\n",
    "|---------------------|--------------------|--------------------|\n",
    "| Actual Positive     | 100                | 50                 |\n",
    "| Actual Negative     | 30                 | 120                |\n",
    "\n",
    "In this matrix,\n",
    "\n",
    "- True Positives (TP) = 100 (The model correctly predicted Positive 100 times)\n",
    "- False Negatives (FN) = 50 (The model incorrectly predicted Negative 50 times when it was actually Positive)\n",
    "- False Positives (FP) = 30 (The model incorrectly predicted Positive 30 times when it was actually Negative)\n",
    "- True Negatives (TN) = 120 (The model correctly predicted Negative 120 times)\n",
    "\n",
    "We can calculate accuracy, recall, and precision with these numbers:\n",
    "\n",
    "- Accuracy: It is simply a ratio of correctly predicted observations to the total observations.\n",
    "  - Accuracy = (TP+TN) / (TP+FP+FN+TN) = (100+120) / (100+30+50+120) = 220 / 300 = 0.73 or 73%\n",
    "- Recall (Sensitivity): It is the ratio of correctly predicted positive observations to the all observations in actual positive class.\n",
    "  - Recall = TP / (TP+FN) = 100 / (100+50) = 100 / 150 = 0.67 or 67%\n",
    "- Precision: It is the ratio of correctly predicted positive observations to the total predicted positive observations.\n",
    "  - Precision = TP / (TP+FP) = 100 / (100+30) = 100 / 130 = 0.77 or 77%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "sns.heatmap(cm, annot=True)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual Labels')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style = \"background-color: yellow\">\n",
    "TODO: With your neighbor, choose individually between precision and recall. Try calculating the precision and recall for your confusion matrix, then share your math with one another.\n",
    "</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c39423ae1c644aadf85ccfd67f8f8d732920ddc5622a7adb6253a26ef962b595"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
