{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA-TimeSeriesAnalysis-Scikit Lesson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation and Relationship Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lesson, we will need the following packages; numpy, pandas, matplotlib, sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation and relationship analysis are important techniques in data science for understanding the association between variables. In this lesson, we will explore the concept of correlation, it's types, and how to analyze relationships between variables using Python. We will use practical examples to demonstrate these concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation measures the statistical relationship between two variables. It helps us understand how changes in one variable are associated with changes in another variable. Correlation does not imply causation, but it indicates the strength and direction of the relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different correlation coefficients used to quantify the relationship between variables:\n",
    "\n",
    "Pearson correlation coefficient (r): It measures the linear relationship between two continuous variables. The value of r ranges from -1 to +1. A positive value indicates a positive linear relationship, a negative value indicates a negative linear relationship, and a value close to zero indicates no linear relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Example data\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([3, 4, 5, 6, 7])\n",
    "\n",
    "# Calculate Pearson correlation coefficient\n",
    "correlation_matrix = np.corrcoef(x, y)\n",
    "\n",
    "# since .corrcoef returns a matrix of the correlations\n",
    "# for different combinations of variables, we choose the index\n",
    "# [0, 1] to access the correlation for variable 1 (0th index) \n",
    "# and variable 2 (1st index)\n",
    "pearson_coefficient = correlation_matrix[0, 1]\n",
    "\n",
    "print(\"Pearson correlation coefficient:\", correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style = \"background-color: yellow\">\n",
    "TODO: Choose 2 numerical variables that you would like to analyze the relationship between. Using our pokemon example, a good choice could be HP and Attack. After choosing the variables, find the correlation coefficient for the 2 variables. Then, explain what your resulting coefficient means to your classmates.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize our correlation by seeing the scatterplot of the variables plotted against one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example data\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([3, 4, 5, 6, 7])\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.scatter(x, y)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Scatter Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When interpreting correlation coefficients, consider the following:\n",
    "\n",
    "Positive Correlation: A positive correlation coefficient indicates that as one variable increases, the other variable tends to increase as well. The closer the value is to +1, the stronger the positive correlation.\n",
    "\n",
    "Negative Correlation: A negative correlation coefficient indicates that as one variable increases, the other variable tends to decrease. The closer the value is to -1, the stronger the negative correlation.\n",
    "\n",
    "No Correlation: A correlation coefficient close to zero indicates no linear relationship between the variables. However, it's important to note that there could still be a non-linear relationship or a relationship that is not captured by the correlation coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation analysis may be affected by missing values and outliers in the data. It's crucial to handle them appropriately:\n",
    "\n",
    "Missing Values: Missing data can lead to biased correlation results. You can handle missing values by imputation techniques (e.g., mean, median, or regression imputation) or by removing observations with missing data, depending on the situation.\n",
    "\n",
    "Outliers: Outliers can have a significant impact on correlation coefficients. Consider identifying and handling outliers before performing correlation analysis. Techniques like winsorization, trimming, or using robust correlation measures can help mitigate the influence of outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next few lessons, we will be diving into machine-earning. Machine-learning is a branch of artificial intelligence (AI) that focuses on developing algorithms and models that enable computers to learn and make predictions or decisions without being explicitly programmed. In simple terms, machine-learning is about teaching computers how to learn from data and use that knowledge to perform tasks or make predictions.\n",
    "\n",
    "The 3 fundamental steps of machine-Learning can be described as first getting your data, then creating a model and fitting it to the existing data, and lastly using the model to make predictions based off of how it is fitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn, also known as sklearn, is a widely-used open-source machine-learning library for Python. It provides a range of efficient tools and algorithms for various machine-learning tasks, including classification, regression, clustering, dimensionality reduction, model selection, and preprocessing of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to load our data set. Sklearn does have its own pre-loaded datasets that we can access. Since all of our datasets come from a source outside sklearn, we access our dataset using pandas like we have done in the past. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load a dataset from a CSV file using Pandas\n",
    "data = pd.read_csv('students_performance.csv')\n",
    "data.columns\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now before we do any actual Machine Learning, we need to prep our data. Machine Learning at heart is using Statistical methods to predict an outcome. We cannot predict anything if our variables are all categorical. Alot of your data sets use Categorical Variables (an example in our StudentsPerformance dataset would be the gender column having a string stating whether someone is female or male) for various columns. One way we can convert these into Quantitative Variables is by using the OneHotEncoder class inside of sklearn. It looks for all the unique values in a column and converts them into respective values. Using our student's example, it may convert male to 0 and female to 1. \n",
    "\n",
    "Here's how you would use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Create an instance of the OneHotEncoder and fit_transform the data\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# store our new data in a variable\n",
    "quantitative_data = encoder.fit_transform(data[['gender']])\n",
    "\n",
    "# Print the transformed data as an array\n",
    "print(quantitative_data.toarray())\n",
    "\n",
    "# Print the encoded feature names\n",
    "print(encoder.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style = \"background-color: yellow\">\n",
    "TODO: Now that we know how to transform Categorical Variables into Quantitative Variables, think about a column you may want to predict and what column or columns you may want to use as your features. Then, if those selected feature columns are not already quantitative variables, convert them.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can split our data into features and the column we are trying to predict. X would be our column(s) that we use as features and y would be the column we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load a dataset from a CSV file using Pandas\n",
    "data = pd.read_csv('students_performance.csv')\n",
    "\n",
    "X = data.iloc[:, :-1].values  # Features\n",
    "# [:, :-1] specifies that we want to select all rows (:) and all columns except the last one (:-1).\n",
    "\n",
    "y = data.iloc[:, -1].values  # Target variable\n",
    "# [:, -1] specifies that we want to select all rows (:) and only the last column (-1).\n",
    "\n",
    "# if we want specific columns, we could replace the -1 with a different index or with brackets around the column names\n",
    "\n",
    "# .values converts the selected data into a NumPy array.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For today, we will be learning about the K-Nearest Neighbors Classifier. The K-Nearest Neighbors (KNN) classifier is a simple and intuitive algorithm for classification tasks. It works based on the idea that similar data points tend to belong to the same class.Imagine you have a dataset with labeled examples of different objects, such as fruits, where each object has certain features like color, shape, and size. The KNN classifier aims to classify new, unlabeled objects based on their similarity to the labeled objects.\n",
    "\n",
    "If we were to look at this from a graphical perspective using our students example, imagine we have a scatterplot with our level of education(X) plotted against math score(y). We have a point with a level of eduction of 'bachelors' that we want to predict the math score of. The KNN Classifier would choose k-points that are closest to this point on the scatterplot and take the average math score of these points to estimate the predicted point's math score.\n",
    "\n",
    "How KNN Works:\n",
    "\n",
    "- Step 1: Select a value for k, which represents the number of nearest neighbors to consider.\n",
    "- Step 2: Given a new, unlabeled object, find the k nearest neighbors to that object in the feature space.\n",
    "- Step 3: Determine the majority class among the k neighbors.\n",
    "- Step 4: Assign the majority class as the predicted class for the new object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how KNN works, we have to split our dataset into training and testing data. Think of training data as the material used to teach a model. It consists of examples or observations with known input values and corresponding output labels or target values. The training data is used to build or train the model. The model learns patterns, relationships, and rules from the training data, enabling it to make predictions or decisions. Testing data is used to assess how well the trained model generalizes to unseen or new data. It serves as a benchmark to evaluate the model's performance and measure it's ability to make accurate predictions on data it hasn't encountered during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# here we are splitting our data into training and testing datasets\n",
    "# the test_size parameter allows us to change what percent of our data we want to designate for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the KNeighborsClassifier with k=3 neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style = \"background-color: yellow\">\n",
    "TODO: Using the knowledge we have about KNN, create a model to predict the values of your column of choice. \n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section we will need the following packages; pandas, matplotlib, numpy, statsmodels, sklearn, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Analysis Basics and loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time series analysis is a sequence of evenly spaced out observations recorded at a specific time interval. \n",
    "\n",
    "Time series analysis involves multiple steps...\n",
    "- 1) Data Collection\n",
    "- 2) Data Preprocessing\n",
    "- 3) Visualization\n",
    "- 4) Stationarity Assessment\n",
    "- 5) Model Selection\n",
    "- 6) Parameter Estimation\n",
    "- 7) Model Fitting and Diagnostic Checking\n",
    "- 8) Forecasting\n",
    "\n",
    "Different sources will have slightly different wording and may merge a few steps together but the essence of TSA(Time-Series analysis) is to achieve an effective model capable of forecasting, pattern recognition, anomaly detection, etc... It's usage could be applied to a wide variety of different positions and fields, basically anywhere that data is collected over time can be a use-case for TSA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stocks = pd.read_csv('sp500_index.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing is to clean the data by handling missing values, outliers, and inconsistencies. This step may also involve transforming the data if necessary, such as taking logarithms or differencing to stabilize the variance. Since time-series data is evenly spaced out filling in these NA values is incredibly important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Handle missing values by filling with the mean\n",
    "data_filled = df_stocks.fillna(df_stocks['S&P500'].mean())\n",
    "\n",
    "# Log transformation to stabilize variance\n",
    "data_log = np.log(df_stocks['S&P500'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the time series data to observe it's overall pattern and identify any apparent trends, seasonality, or other patterns. Visualization can provide insights into the underlying behavior of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the time series\n",
    "plt.plot(df_stocks['S&P500'])\n",
    "plt.title('S&P 500 over time')\n",
    "plt.xlabel('Time(days)')\n",
    "plt.ylabel('Value(points)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stationarity Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the time series data is stationary, meaning it has a constant mean and variance over time. Stationarity is an important assumption for many time series models. If the data is not stationary, it may require transformation or differencing to achieve stationarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Augmented Dickey-Fuller test for stationarity\n",
    "result = adfuller(df_stocks['S&P500'])\n",
    "\n",
    "# Check p-value for stationarity\n",
    "p_value = result[1]\n",
    "if p_value < 0.05:\n",
    "    print('Time series is stationary')\n",
    "else:\n",
    "    print('Time series is non-stationary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data is non-stationary, we must find a method to stabilize the variance and the mean over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 1: Log Transformation\n",
    "If the data exhibits exponential growth or has a varying variance, applying a logarithmic transformation can help stabilize the variance and make it exhibit higher stationarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving forward, we will be using google colab to run our code. You can access google colab with this link: \n",
    "https://www.bing.com/ck/a?!&&p=27045da591c34ac2JmltdHM9MTY4OTM3OTIwMCZpZ3VpZD0xZTEwMzZkMS04MzQ1LTYwZTAtMWYwZi0yNjFkODI2YzYxODQmaW5zaWQ9NTE5MA&ptn=3&hsh=3&fclid=1e1036d1-8345-60e0-1f0f-261d826c6184&psq=google+colaboratory&u=a1aHR0cHM6Ly9jb2xhYi5yZXNlYXJjaC5nb29nbGUuY29tLw&ntb=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation and Relationship Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation and relationship analysis are important techniques in data science for understanding the association between variables. In this lesson, we will explore the concept of correlation, it's types, and how to analyze relationships between variables using Python. We will use practical examples to demonstrate these concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation measures the statistical relationship between two variables. It helps us understand how changes in one variable are associated with changes in another variable. Correlation does not imply causation, but it indicates the strength and direction of the relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different correlation coefficients used to quantify the relationship between variables:\n",
    "\n",
    "Pearson correlation coefficient (r): It measures the linear relationship between two continuous variables. The value of r ranges from -1 to +1. A positive value indicates a positive linear relationship, a negative value indicates a negative linear relationship, and a value close to zero indicates no linear relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Example data\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([3, 4, 5, 6, 7])\n",
    "\n",
    "# Calculate Pearson correlation coefficient\n",
    "correlation_matrix = np.corrcoef(x, y)\n",
    "\n",
    "# since .corrcoef returns a matrix of the correlations\n",
    "# for different combinations of variables, we choose the index\n",
    "# [0, 1] to access the correlation for variable 1 (0th index) \n",
    "# and variable 2 (1st index)\n",
    "pearson_coefficient = correlation_matrix[0, 1]\n",
    "\n",
    "print(\"Pearson correlation coefficient:\", correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style = \"background-color: yellow\">\n",
    "TODO: Choose 2 numerical variables that you would like to analyze the relationship between. Using our pokemon example, a good choice could be HP and Attack. After choosing the variables, find the correlation coefficient for the 2 variables. Then, explain what your resulting coefficient means to your classmates.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize our correlation by seeing the scatterplot of the variables plotted against one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example data\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([3, 4, 5, 6, 7])\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.scatter(x, y)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Scatter Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When interpreting correlation coefficients, consider the following:\n",
    "\n",
    "Positive Correlation: A positive correlation coefficient indicates that as one variable increases, the other variable tends to increase as well. The closer the value is to +1, the stronger the positive correlation.\n",
    "\n",
    "Negative Correlation: A negative correlation coefficient indicates that as one variable increases, the other variable tends to decrease. The closer the value is to -1, the stronger the negative correlation.\n",
    "\n",
    "No Correlation: A correlation coefficient close to zero indicates no linear relationship between the variables. However, it's important to note that there could still be a non-linear relationship or a relationship that is not captured by the correlation coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation analysis may be affected by missing values and outliers in the data. It's crucial to handle them appropriately:\n",
    "\n",
    "Missing Values: Missing data can lead to biased correlation results. You can handle missing values by imputation techniques (e.g., mean, median, or regression imputation) or by removing observations with missing data, depending on the situation.\n",
    "\n",
    "Outliers: Outliers can have a significant impact on correlation coefficients. Consider identifying and handling outliers before performing correlation analysis. Techniques like winsorization, trimming, or using robust correlation measures can help mitigate the influence of outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next few lessons, we will be diving into machine learning. Machine learning is a branch of artificial intelligence (AI) that focuses on developing algorithms and models that enable computers to learn and make predictions or decisions without being explicitly programmed. In simple terms, machine learning is about teaching computers how to learn from data and use that knowledge to perform tasks or make predictions.\n",
    "\n",
    "The 3 fundamental steps of Machine Learning can be described as first getting your data, then creating a model and fitting it to the existing data, and lastly using the model to make predictions based off of how it is fitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn, also known as sklearn, is a widely-used open-source machine learning library for Python. It provides a range of efficient tools and algorithms for various machine learning tasks, including classification, regression, clustering, dimensionality reduction, model selection, and preprocessing of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to load our data set. Sklearn does have it's own pre-loaded datasets that we can access. Since all of our datasets come from a source outside sklearn, we access our dataset using pandas like we have done in the past. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load a dataset from a CSV file using Pandas\n",
    "data = pd.read_csv('StudentsPerformance.csv')\n",
    "data.columns\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now before we do any actual Machine Learning, we need to prep our data. Machine Learning at heart is using Statistical methods to predict an outcome. We cannot predict anything if our variables are all categorical. Alot of your data sets use Categorical Variables (an example in our StudentsPerformance dataset would be the gender column having a string stating whether someone is female or male) for various columns. One way we can convert these into Quantitative Variables is by using the OneHotEncoder class inside of sklearn. It looks for all the unique values in a column and converts them into respective values. Using our student example, it may convert male to 0 and female to 1. \n",
    "\n",
    "Here's how you would use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Create an instance of the OneHotEncoder and fit_transform the data\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# store our new data in a variable\n",
    "quantitative_data = encoder.fit_transform(data[['gender']])\n",
    "\n",
    "# Print the transformed data as an array\n",
    "print(quantitative_data.toarray())\n",
    "\n",
    "# Print the encoded feature names\n",
    "print(encoder.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style = \"background-color: yellow\">\n",
    "TODO: Now that we know how to transform Categorical Variables into Quantitative Variables, think about a column you may want to predict and what column or columns you may want to use as your features. Then, if those selected feature columns are not already quantitative variables, convert them.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can split our data into features and the column we are trying to predict. X would be our column(s) that we use as features and y would be the column we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load a dataset from a CSV file using Pandas\n",
    "data = pd.read_csv('StudentsPerformance.csv')\n",
    "\n",
    "X = data.iloc[:, :-1].values  # Features\n",
    "# [:, :-1] specifies that we want to select all rows (:) and all columns except the last one (:-1).\n",
    "\n",
    "y = data.iloc[:, -1].values  # Target variable\n",
    "# [:, -1] specifies that we want to select all rows (:) and only the last column (-1).\n",
    "\n",
    "# if we want specific columns, we could replace the -1 with a different index or with brackets around the column names\n",
    "\n",
    "# .values converts the selected data into a NumPy array.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For today, we will be learning about the K-Nearest Neighbors Classifier. The K-Nearest Neighbors (KNN) classifier is a simple and intuitive algorithm for classification tasks. It works based on the idea that similar data points tend to belong to the same class.Imagine you have a dataset with labeled examples of different objects, such as fruits, where each object has certain features like color, shape, and size. The KNN classifier aims to classify new, unlabeled objects based on their similarity to the labeled objects.\n",
    "\n",
    "If we were to look at this from a graphical perspective using our students example, imagine we have a scatterplot with our level of education(X) plotted against math score(y). We have a point with a level of eduction of 'bachelors' that we want to predict the math score of. The KNN Classifier would choose k-points that are closest to this point on the scatterplot and take the average math score of these points to estimate the predicted point's math score.\n",
    "\n",
    "How KNN Works:\n",
    "\n",
    "- Step 1: Select a value for k, which represents the number of nearest neighbors to consider.\n",
    "- Step 2: Given a new, unlabeled object, find the k nearest neighbors to that object in the feature space.\n",
    "- Step 3: Determine the majority class among the k neighbors.\n",
    "- Step 4: Assign the majority class as the predicted class for the new object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how KNN works, we have to split our dataset into training and testing data. Think of training data as the material used to teach a model. It consists of examples or observations with known input values and corresponding output labels or target values. The training data is used to build or train the model. The model learns patterns, relationships, and rules from the training data, enabling it to make predictions or decisions. Testing data is used to assess how well the trained model generalizes to unseen or new data. It serves as a benchmark to evaluate the model's performance and measure it's ability to make accurate predictions on data it hasn't encountered during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# here we are splitting our data into training and testing datasets\n",
    "# the test_size parameter allows us to change what percent of our data we want to designate for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the KNeighborsClassifier with k=3 neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style = \"background-color: yellow\">\n",
    "TODO: Using the knowledge we have about KNN, create a model to predict the values of your column of choice. \n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differencing to achieve stationarity\n",
    "stationary_data = df_stocks['S&P500'].diff().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = adfuller(stationary_data)\n",
    "\n",
    "# Check p-value for stationarity\n",
    "p_value = result[1]\n",
    "if p_value < 0.05:\n",
    "    print('Time series is stationary')\n",
    "else:\n",
    "    print('Time series is non-stationary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data has stationarity we can move onto the next step which would be selecting a model to best represent our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select an appropriate time series model based on the characteristics of the data. Common models include autoregressive integrated moving average (ARIMA), exponential smoothing methods, and state space models. The choice of model depends on the presence of trends, seasonality, and other specific characteristics observed in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Fit an ARIMA model\n",
    "model = ARIMA(stationary_data, order=(1, 1, 1))\n",
    "model_fit = model.fit(method_kwargs={'optimizer_kwargs': {'disp': 0}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate the parameters of the chosen model using various estimation techniques, such as maximum likelihood estimation (MLE) or least squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate parameters using maximum likelihood estimation\n",
    "model_fit = model.fit(method_kwargs={'optimizer_kwargs': {'disp': 0}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fitting and Diagnostic Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model to the data and evaluate its performance. This step involves checking the residuals for patterns or systematic deviations from randomness. Diagnostic tests help ensure that the model assumptions are met and the model adequately captures the data patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot residuals\n",
    "residuals = model_fit.resid\n",
    "plt.plot(stationary_data)\n",
    "plt.title('Residuals')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Residual')\n",
    "plt.show()\n",
    "\n",
    "# Perform Ljung-Box test for residual randomness\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "\n",
    "# Calculate Ljung-Box test statistics and p-values\n",
    "lb_value, p_value = acorr_ljungbox(residuals)\n",
    "\n",
    "# Check p-value for residual randomness\n",
    "if all(p > 0.05 for p in p_value):\n",
    "    print('Residuals are random')\n",
    "else:\n",
    "    print('Residuals have patterns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the fitted model to make predictions about future values of the time series. Forecasting can provide valuable insights for decision-making and planning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for future values\n",
    "forecast = model_fit.forecast(steps=10)\n",
    "\n",
    "# Print forecasted values\n",
    "print(forecast)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
