{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving forward, we will be using google colab to run our code. You can access google colab with this link: \n",
    "https://www.bing.com/ck/a?!&&p=27045da591c34ac2JmltdHM9MTY4OTM3OTIwMCZpZ3VpZD0xZTEwMzZkMS04MzQ1LTYwZTAtMWYwZi0yNjFkODI2YzYxODQmaW5zaWQ9NTE5MA&ptn=3&hsh=3&fclid=1e1036d1-8345-60e0-1f0f-261d826c6184&psq=google+colaboratory&u=a1aHR0cHM6Ly9jb2xhYi5yZXNlYXJjaC5nb29nbGUuY29tLw&ntb=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section we will need the following packages; pandas, matplotlib, numpy, statsmodels, sklearn, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Analysis Basics and loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time series analysis is a sequence of evenly spaced out observations recorded at a specific time interval. \n",
    "\n",
    "Time series analysis involves multiple steps...\n",
    "- 1) Data Collection\n",
    "- 2) Data Preprocessing\n",
    "- 3) Visualization\n",
    "- 4) Stationarity Assessment\n",
    "- 5) Model Selection\n",
    "- 6) Parameter Estimation\n",
    "- 7) Model Fitting and Diagnostic Checking\n",
    "- 8) Forecasting\n",
    "\n",
    "Different sources will have slightly different wording and may merge a few steps together but the essence of TSA(Time-Series analysis) is to achieve an effective model capable of forecasting, pattern recognition, anomaly detection, etc... It's usage could be applied to a wide variety of different positions and fields, basically anywhere that data is collected over time can be a use-case for TSA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stocks = pd.read_csv('sp500_index.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing is cleaning the data by handling missing values, outliers, and inconsistencies. This step may also involve transforming the data if necessary, such as taking logarithms or differencing to stabilize the variance. Since time-series data is evenly spaced out filling in these NA values is incredibly important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Handle missing values by filling with the mean\n",
    "data_filled = df_stocks.fillna(df_stocks['S&P500'].mean())\n",
    "\n",
    "# Log transformation to stabilize variance\n",
    "data_log = np.log(df_stocks['S&P500'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style =\"background-color:yellow\">\n",
    "TODO: Spend a couple minutes cleanign your data and prepping it for further processing. Why is important to clean out data for time series analysis(TSA)? When answering this question think about what the goal of TSA is. Discuss these topics with a partner. \n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the time series data to observe it's overall pattern and identify any apparent trends, seasonality, or other patterns. Visualization can provide insights into the underlying behavior of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the time series\n",
    "plt.plot(df_stocks['S&P500'])\n",
    "plt.title('S&P 500 over time')\n",
    "plt.xlabel('Time(days)')\n",
    "plt.ylabel('Value(points)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation and Relationship Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation and relationship analysis are important techniques in data science for understanding the association between variables. In this lesson, we will explore the concept of correlation, its types, and how to analyze relationships between variables using Python. We will use practical examples to demonstrate these concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation measures the statistical relationship between two variables. It helps us understand how changes in one variable are associated with changes in another variable. Correlation does not imply causation, but it indicates the strength and direction of the relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different correlation coefficients used to quantify the relationship between variables:\n",
    "\n",
    "Pearson correlation coefficient (r): It measures the linear relationship between two continuous variables. The value of r ranges from -1 to +1. A positive value indicates a positive linear relationship, a negative value indicates a negative linear relationship, and a value close to zero indicates no linear relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Example data\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([3, 4, 5, 6, 7])\n",
    "\n",
    "# Calculate Pearson correlation coefficient\n",
    "correlation_matrix = np.corrcoef(x, y)\n",
    "\n",
    "# since .corrcoef returns a matrix of the correlations\n",
    "# for different combinations of variables, we choose the index\n",
    "# [0, 1] to access the correlation for variable 1 (0th index) \n",
    "# and variable 2 (1st index)\n",
    "pearson_coefficient = correlation_matrix[0, 1]\n",
    "\n",
    "print(\"Pearson correlation coefficient:\", correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style = \"background-color: yellow\">\n",
    "TODO: Choose 2 numerical variables that you would like to analyze the relationship between. Using our pokemon example, a good choice could be HP and Attack. After choosing the variables, find the correlation coefficient for the 2 variables. Then, explain what your resulting coefficient means to your classmates.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize our correlation by seeing the scatterplot of the variables plotted against one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example data\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([3, 4, 5, 6, 7])\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.scatter(x, y)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Scatter Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When interpreting correlation coefficients, consider the following:\n",
    "\n",
    "Positive Correlation: A positive correlation coefficient indicates that as one variable increases, the other variable tends to increase as well. The closer the value is to +1, the stronger the positive correlation.\n",
    "\n",
    "Negative Correlation: A negative correlation coefficient indicates that as one variable increases, the other variable tends to decrease. The closer the value is to -1, the stronger the negative correlation.\n",
    "\n",
    "No Correlation: A correlation coefficient close to zero indicates no linear relationship between the variables. However, it's important to note that there could still be a non-linear relationship or a relationship that is not captured by the correlation coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation analysis may be affected by missing values and outliers in the data. It's crucial to handle them appropriately:\n",
    "\n",
    "Missing Values: Missing data can lead to biased correlation results. You can handle missing values by imputation techniques (e.g., mean, median, or regression imputation) or by removing observations with missing data, depending on the situation.\n",
    "\n",
    "Outliers: Outliers can have a significant impact on correlation coefficients. Consider identifying and handling outliers before performing correlation analysis. Techniques like winsorization, trimming, or using robust correlation measures can help mitigate the influence of outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stationarity Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson we will be learning why we need to perform a stationarity test and how it will effect our model. When we use a stationarity test we can find trends in the pattern which tell us if our data has a consistent flow and what the pattern inthe data indicates. This can be done in three major components: Trend: which identifies the movement or direction in the data, Seasonality: The repeating patterns or cycles that occur on annual intervals in data, Residuals: which are the random fluctuations or irregularity in our data.\n",
    "\n",
    "We also need to identify what it means for a time series to be considered stationary. To be considered stationary it would need to satisfy these conditions: a constant mean, a constant variance, and lastly a constant autocovariance.\n",
    "\n",
    "So we know that the stationarity test is used to identify key patterns in the data and to simplify the inconsistencies in our data to make it easier to forcast and model.\n",
    "\n",
    "Check if the time series data is stationary, meaning it has a constant mean and variance over time. Stationarity is an important assumption for many time series models. If the data is not stationary, it may require transformation or differencing to achieve stationarity.\n",
    "\n",
    "There are multiple different tests to check for stationarity.\n",
    "- Dickey-Fuller test\n",
    "- Augmented Dickey-Fuller test\n",
    "- Kwiatkowski-Phillips-Schmidt-Shin (KPSS) Test\n",
    "- Phillips-Perron (PP) Test\n",
    "\n",
    "\n",
    "**Rolling Statistics** takes the moving average or moving variance and sees it vary with time. It is more of a visual technique and would not be practical for production.\n",
    "\n",
    "**Dickey-Fuller Test(DF)** assesses whether a unit root is present in a time series, which is an indicator of non-stationarity. The DF test is a statistical significance test which means that there is hypothesis testing involved. The null hypothesis is rejected if the data is considered stationary. There is further complexity to this process and there is something known as the Augmented Dickey-Fuller test but for out purposes this is all we need to know for now.The DF test is also one of the most commonly used test in analyzing stationarity of a series.\n",
    "- **Unit Root** is an indicator that past values of the time series influence current values. This is an example non-stationary data which will negatively influence the effectivness of your model.\n",
    "\n",
    "**Augmented Dickey-Fuller Test(ADF)** is a hypothesis test which takes a higher order of variance into account to determine if the data is stationary, requires transformation or if the results are inconclusive. Main difference between the DF and the ADF test is that the ADF test can be used on large ordee sized data since it has more differencing terms. Otherwise the null hypothesis remains the same.\n",
    "\n",
    "**Kwiatkowski-Phillips-Schmidt-Shin (KPSS) Test** is another popular test to examine stationarity. It works in the opposite way to the ADF test by testing the null hypothesis of stationarity against the alternative hypothesis of a unit root (non-stationarity).\n",
    "\n",
    "**Phillips-Perron (PP) Test** is similar to the ADF test but uses a different estimation method. It tests for the presence of a unit root and is commonly used when the underlying data might have autocorrelation (the statistical relationship between data points) or heteroscedasticity(variablility of a dependant variable that differs across independant variables).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our purposes we are using the Dickey-Fuller test since we want to analyze a fairly large amount of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import adfuller module from the statsmodels package\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Augmented Dickey-Fuller test for stationarity\n",
    "result = adfuller(df_stocks['S&P500'])\n",
    "\n",
    "# Check p-value for stationarity\n",
    "p_value = result[1]\n",
    "if p_value < 0.05:\n",
    "    print('Time series is stationary')\n",
    "else:\n",
    "    print('Time series is non-stationary')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data is non-stationary, we must find a method to stabilize the variance and the mean over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When turning non-stationary data into stationary data you must remove the underlying trends and seasonality in the data. The most common ways to do this are...\n",
    "\n",
    "- Differencing\n",
    "- Log Transformation\n",
    "- Seasonal Adjustment\n",
    "- Smoothing\n",
    "- Detrending\n",
    "- Polynomial Fitting\n",
    "\n",
    "To determine where to start analyze your data visually to notice patterns, exponential growth or decay, or more complex curves in the data. This can help determine how to fix that such as with diffeerencing or with a log transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since out data is non stationary and visually it has a slight exponential growth component to it we can use a Log Transformation to hopefully make it stationary.\n",
    "\n",
    "If the data exhibits exponential growth or has a varying variance, applying a logarithmic transformation can help stabilize the variance and make it stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differencing to achieve stationarity\n",
    "stationary_data = df_stocks['S&P500'].diff().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = adfuller(stationary_data)\n",
    "\n",
    "# Check p-value for stationarity\n",
    "p_value = result[1]\n",
    "if p_value < 0.05:\n",
    "    print('Time series is stationary')\n",
    "else:\n",
    "    print('Time series is non-stationary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data has stationarity we can move on to the next step which would be selecting a model to best represent our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style = \"background-color: yellow\">\n",
    "TODO: Check you data set for stationarity. If it is not, determine what may need to be done to make it stationary. Look at your data again after your transformation and write down in a couple sentences the differences you see. Discuss with a fellow classmate your findings for a couple minutes.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factors for choosing a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know the data is clean, that there is a trend, and that it is has stationarity it is now time to select a model to feed our data into. The type of model you choose is based on what kind of parameters you want set and the forecasting requirements that are necessitated. \n",
    "\n",
    "A few important factors to keep in mind when selecting a model are ...\n",
    "- Trend\n",
    "- Seasonality\n",
    "- Complex Seasonality\n",
    "- Stationarity\n",
    "- Outliers\n",
    "- Forecast Horizon\n",
    "- Data Volume\n",
    "- Handling External Factors\n",
    "- Uncertainty Estimation\n",
    "- Model Complexity\n",
    "- Accuracy vs Interpretability\n",
    "\n",
    "These are just a few of the many potential factors determining the model you may use. Spend time researching each component as the choice you make greatly impacts the model's accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different types of Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an idea of what to look for in our data to determine what model to use, lets choose a model. The Autoregressive Integrated Moving Average(ARIMA) model is widely used and it combines autoregression and moving average components while incorporating differencing to make the data stationary. The definition is a bit complex so breaking it down will help with understanding.\n",
    "\n",
    "- Autoregression: Means that the future values of the time series are influenced by past values. A great way to summarize it is that the past influences the future.\n",
    "- Moving Average(MA): Means that the future values are influenced by past forecast errors. Forecast errors are the differences between predicted values and the actual values. This can also be known as the residual value or loss in ML.\n",
    "- Differencing(Integrated): This is all about making the data easier to use within the model or stationary. Stationary data has a constant mean and variance over time. If the data for example has an increasing trend instead of being stationary then the we difference the data. This is just subtracting each value from its previous value. This removes the trend and makes the data stationary.\n",
    "\n",
    "ARIMA works the best when there is a straight line trend or seasonality within the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Fit an ARIMA model\n",
    "model = ARIMA(stationary_data, order=(1, 1, 1))\n",
    "model_fit = model.fit(method_kwargs={'optimizer_kwargs': {'disp': 0}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example we use the ARIMA model. \n",
    "\n",
    "After importing the model using `from statsmodels.tsa.arima.model import ARIMA`, we can now populate it with the order of the model. Order in this case is referring to `p`, `d`, and `q` which are the autoregressive, differencing, and moving average components respectively. \n",
    "\n",
    "Order determines the structure and behavior of the model. In this case we filled each value with 1 indicating that each of the 3 components will have a value of 1. In some cases you may want only a moving average component to the model in which you would a value of 1 for `q` and a value of 0 for `p` and `d`. \n",
    "\n",
    "The last line of this section `model_fit = model.fit(method_kwargs={'optimizer_kwargs': {'disp': 0}})` has to do with fitting the ARIMA model to the data we provided, in this case `stationary_data`. The `method_kwargs` is used to control how the model is estimated. `'optimizer_kwargs'` is used to modify the behavior of the optimization algorithm which is basically choosing the best parameters for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the ARIMA model due to its flexibility and interpretability relative to other models. However other models to exist such as...\n",
    "\n",
    "- Seasonal Autoregression Integrated Moving Average(SARIMA): Is like ARIMA but includes a seasonality component.\n",
    "- Exponential Smoothing(ETS): Use weighted averages of past observations to predict future values. There are other variants within ETS models to provide further tuning.\n",
    "- Seasonal Decomposition of Time Series(STL): Decomposes time series into three components: seasonal, trend, and residual. \n",
    "\n",
    "There are many more models out there that provide further fine tuning. Being aware of when and how to use them will make your forecasting that much more accurate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style = \"background-color: yellow\">\n",
    "TODO: Although the ARIMA model is robust and flexible spend a couple minutes looking up one other model. Write down 3-4 sentences about the big differences this new model has compared to the ARIMA model. Based off this new information would this new model be better for forecasting for your data? Discuss with a partner why it may or may not be better.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro to Hyperparameter Tuning and Feature Engineering\n",
    "Feature engineering and hyperparameter tuning are two important steps in the machine learning model development process, and they are often performed sequentially to optimize model performance.\n",
    "Feature engineering involves the process of transforming raw data into a format that is suitable for model training. The goal of feature engineering is to extract relevant information from the data and create informative features (input variables) that will help the model learn patterns and make accurate predictions. Good features can significantly improve a model's performance, even before hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style = \"background-color: yellow\">\n",
    "TODO: Begin to put yourself into the shoes of a data scientist. Create a new feature in your dataset that is based off of other variables in your data. Make sure this feature at least seems useful when it comes to predicting your target variable. Hint: Think back to when we created a new column.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effective feature engineering can reduce the complexity of the model and provide the model with more relevant information, leading to improved performance. Once the features are engineered, the next step is hyperparameter tuning to optimize the model's settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a model is fitted, hyperparameters are assigned values. In the context of statistical and machine learning models, a hyperparameter is a numerical value that defines certain characteristics or properties of the model. Hyperparameters play a crucial role in determining how the model behaves and how it interacts with the data during the fitting or training process. Different models have different sets of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often times, the assigned values of the hyperparameters are not optimized to accurately reflect patterns in the data. The model could be susceptible to bias, factor in noise data too heavily, and or make other incorrect conclusions about the data. To prevent this, we adjust the values that the model assigns to the hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using the ARIMA model, our hyperparameters are as follows:\n",
    "\n",
    "- p: The order of the autoregressive component (AR) representing the relationship between the current observation and its previous values.\n",
    "- d: The degree of differencing needed to make the time series stationary.\n",
    "- q: The order of the moving average component (MA) representing the relationship between the current observation and the past error terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the optimal parameter values for an ARIMA model involves a process called hyperparameter tuning. The goal is to choose the values of (p, d, q) that result in the best performance of the model on the training data. There are several methods to achieve this, such as using grid search, random search, or more advanced optimization algorithms like Bayesian optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fitting and Diagnostic Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is fitted and has been run it is time to evaluate its performance. This step involves checking the residuals for patterns or systematic deviations from randomness. Diagnostic tests help ensure that the model assumptions are met and the model adequately captures the data patterns. \n",
    "\n",
    "Some different ways the model can be assessed are...\n",
    "\n",
    "- Residual Analysis\n",
    "- ACF(Autocorrelation Function) plot\n",
    "- Ljung-Box Test for Autocorrelation\n",
    "- Histogram and Q-Q plot of residuals\n",
    "\n",
    "In essence each one of these offers another way to examine the effectivness and reliability of your model. In the example below we use the Ljung-Box Test for Autocorrelation which assesses whether the residuals exhibit significant autocorrelation at different lags. \n",
    "\n",
    "**Autocorrelation** is a statistical concept that basically measures how well the values of a time series at different time points related to one another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot residuals\n",
    "residuals = model_fit.resid\n",
    "plt.plot(stationary_data)\n",
    "plt.title('Residuals')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Residual')\n",
    "plt.show()\n",
    "\n",
    "# Perform Ljung-Box test for residual randomness\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "\n",
    "# Calculate Ljung-Box test statistics and p-values\n",
    "lb_value, p_value = acorr_ljungbox(residuals)\n",
    "\n",
    "# Check p-value for residual randomness\n",
    "if all(p > 0.05 for p in p_value):\n",
    "    print('Residuals are random')\n",
    "else:\n",
    "    print('Residuals have patterns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first step we get all the residuals from the data and plot where the x axis is time and y value is the residual. Residuals can be negative and positive. When importing the `acorr_ljungbox` from `statsmodels.stats.diagnostic` we are able to byass calculating the test statistic which in this case is the sum of the squared autocorrelations. The value returned when passing our residuals through the `acorr_ljungbox()` function is the `p_value` or what we test to either accept or reject the null hypothesis. The null hypothesis in the Ljung-Box test is that the residuals are random indicating that the model is a good fit for the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style = \"background-color: yellow\">\n",
    "TODO: After performing a diagnostic check on your data determine whether the residuals are random or not. Why are random residuals important when assessing if the model is a good fit for the data? Write does your reasoning in a couple sentences. \n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forecasting is the final step in figuring out what your predictions look like. However, it is normal to look at your forecast and be unsatisfied. Many times you may go back and tune for additional accuracy and in turn make your forecasts even better. The further out your forecast goes the less accurate it will be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the fitted model to make predictions about future values of the time series. Forecasting can provide valuable insights for decision-making and planning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for future values\n",
    "forecast = model_fit.forecast(steps=10)\n",
    "\n",
    "# Print forecasted values\n",
    "print(forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style = \"background-color: yellow\">\n",
    "TODO: After forecasting your new data points what are some conclusions you can make? Are they what you expected? Looking at your data what other factors would be important in making better forecasts? Discuss with a partner for a couple minutes about your findings.\n",
    "</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
